{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "repnet_colab.ipynb의 사본",
      "provenance": [],
      "collapsed_sections": [
        "jgrs0nUjTwNJ",
        "oFC7Y-egr-8E",
        "OSpVZ5qsaz7Z"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seawavve/Network_wavve/blob/main/RepNet/Counter_TSM_Standazation_baseModelToEfficient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjRn1KazJQqn"
      },
      "source": [
        "Copyright 2020 Google LLC.\n",
        "\n",
        "Licensed aunder the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wHZQpP65JGvj"
      },
      "source": [
        "#@title License\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0iMnWR_nk9E"
      },
      "source": [
        "# RepNet\n",
        "\n",
        "This colab contains a pre-trained [RepNet](https://sites.google.com/view/repnet) model. It can be used to predict the rate at which repetitions are happening in a video in a class-agnostic manner. These estimates can be used to count the number of repetitions in videos.\n",
        "\n",
        "This model is able to count repetitions in many domains: counting the number of reps while exercising, measuring the rate of biological events like heartrates etc. \n",
        "\n",
        "Ensure you are running the Colab with a GPU runtime.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3i6Lyvg0VfT"
      },
      "source": [
        "# Setup\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RqPYBFtrrfcm",
        "outputId": "96a3eb1a-483f-4e0c-96d6-f4a3b282074f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRF_Yk6mzBfB"
      },
      "source": [
        "#@title\n",
        "import base64\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "\n",
        "import cv2\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import HTML\n",
        "from IPython.display import Javascript\n",
        "\n",
        "import matplotlib\n",
        "from matplotlib.animation import FuncAnimation\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from scipy.signal import medfilt\n",
        "\n",
        "import tensorflow.compat.v2 as tf\n",
        "import tensorflow as originTF\n",
        "\n",
        "from google.colab import drive\n",
        "from google.colab import output\n",
        "from google.colab.output import eval_js\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "# Model definition\n",
        "layers = tf.keras.layers\n",
        "regularizers = tf.keras.regularizers\n",
        "FRAME=3950"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76L5XFonl_Bw"
      },
      "source": [
        "class ResnetPeriodEstimator(tf.keras.models.Model):\n",
        "  \"\"\"RepNet model.\"\"\"\n",
        "#DROPOUT_RATE=0.25 -> 0.7\n",
        "#TRANSFORMER_DROPOUT_RATE=0.0 -> 0.7\n",
        "  def __init__(\n",
        "      self,\n",
        "      num_frames=FRAME,\n",
        "      image_size=112,\n",
        "      base_model_layer_name='block5c_activation',\n",
        "      temperature=13.544,\n",
        "      dropout_rate=0.6,\n",
        "      l2_reg_weight=1e-6,\n",
        "      temporal_conv_channels=512,\n",
        "      temporal_conv_kernel_size=3,\n",
        "      temporal_conv_dilation_rate=3,\n",
        "      conv_channels=32,\n",
        "      conv_kernel_size=3,\n",
        "      transformer_layers_config=((512, 4, 512),),\n",
        "      transformer_dropout_rate=0.5,\n",
        "      transformer_reorder_ln=True,\n",
        "      period_fc_channels=(512, 512),\n",
        "      within_period_fc_channels=(512, 512)):\n",
        "    super(ResnetPeriodEstimator, self).__init__()\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    # Model params.\n",
        "    self.num_frames = num_frames\n",
        "    self.image_size = image_size\n",
        "\n",
        "    self.base_model_layer_name = base_model_layer_name\n",
        "\n",
        "    self.temperature = temperature\n",
        "\n",
        "    self.dropout_rate = dropout_rate\n",
        "    self.l2_reg_weight = l2_reg_weight\n",
        "\n",
        "    self.temporal_conv_channels = temporal_conv_channels\n",
        "    self.temporal_conv_kernel_size = temporal_conv_kernel_size\n",
        "    self.temporal_conv_dilation_rate = temporal_conv_dilation_rate\n",
        "\n",
        "    self.conv_channels = conv_channels\n",
        "    self.conv_kernel_size = conv_kernel_size\n",
        "    # Transformer config in form of (channels, heads, bottleneck channels).\n",
        "    self.transformer_layers_config = transformer_layers_config\n",
        "    self.transformer_dropout_rate = transformer_dropout_rate\n",
        "    self.transformer_reorder_ln = transformer_reorder_ln\n",
        "\n",
        "    self.period_fc_channels = period_fc_channels\n",
        "    self.within_period_fc_channels = within_period_fc_channels\n",
        "\n",
        "  ####################################################\n",
        "    # Convolutional feature extractor Base ResNet50 Model.\n",
        "    # 112x112x3 → 7x7x1024\n",
        "    base_model = tf.keras.applications.EfficientNetB0(\n",
        "        include_top=False, weights=None, pooling='max')\n",
        "\n",
        "\n",
        "\n",
        "    self.base_model = tf.keras.models.Model(\n",
        "        inputs=base_model.input,\n",
        "        outputs=base_model.get_layer(self.base_model_layer_name).output)\n",
        "  #####################################################\n",
        "    # Temporal Context\n",
        "    # 3x3x3 512filters with RELU\n",
        "    # 3D Conv on k Frames\n",
        "    self.temporal_conv_layers = [\n",
        "        layers.Conv3D(self.temporal_conv_channels,\n",
        "                      self.temporal_conv_kernel_size,\n",
        "                      padding='same',\n",
        "                      dilation_rate=(self.temporal_conv_dilation_rate, 1, 1),\n",
        "                      kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
        "                      kernel_initializer='he_normal')]\n",
        "    self.temporal_bn_layers = [layers.BatchNormalization()\n",
        "                               for _ in self.temporal_conv_layers]\n",
        "\n",
        "    # Counting Module (Self-sim > Conv > Transformer > Classifier)\n",
        "    self.conv_3x3_layer = layers.Conv2D(self.conv_channels,\n",
        "                                        self.conv_kernel_size,\n",
        "                                        padding='same',\n",
        "                                        activation=tf.nn.relu)\n",
        "\n",
        "    channels = self.transformer_layers_config[0][0]\n",
        "    self.input_projection = layers.Dense(\n",
        "        channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
        "        activation=None)\n",
        "    self.input_projection2 = layers.Dense(\n",
        "        channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
        "        activation=None)\n",
        "\n",
        "    length = self.num_frames\n",
        "    self.pos_encoding = tf.compat.v1.get_variable(\n",
        "        name='resnet_period_estimator/pos_encoding',\n",
        "        shape=[1, length, 1],\n",
        "        initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
        "    self.pos_encoding2 = tf.compat.v1.get_variable(\n",
        "        name='resnet_period_estimator/pos_encoding2',\n",
        "        shape=[1, length, 1],\n",
        "        initializer=tf.compat.v1.truncated_normal_initializer(stddev=0.02))\n",
        "\n",
        "    self.transformer_layers = []\n",
        "    for d_model, num_heads, dff in self.transformer_layers_config:\n",
        "      self.transformer_layers.append(\n",
        "          TransformerLayer(d_model, num_heads, dff,\n",
        "                           self.transformer_dropout_rate,\n",
        "                           self.transformer_reorder_ln))\n",
        "\n",
        "    self.transformer_layers2 = []\n",
        "    for d_model, num_heads, dff in self.transformer_layers_config:\n",
        "      self.transformer_layers2.append(\n",
        "          TransformerLayer(d_model, num_heads, dff,\n",
        "                           self.transformer_dropout_rate,\n",
        "                           self.transformer_reorder_ln))\n",
        "\n",
        "    # Period Prediction Module.\n",
        "    self.dropout_layer = layers.Dropout(self.dropout_rate)\n",
        "    num_preds = self.num_frames//2\n",
        "    self.fc_layers = []\n",
        "    for channels in self.period_fc_channels:\n",
        "      self.fc_layers.append(layers.Dense(\n",
        "          channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
        "          activation=tf.nn.relu))\n",
        "    self.fc_layers.append(layers.Dense(\n",
        "        num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\n",
        "\n",
        "    # Within Period Module\n",
        "    num_preds = 1\n",
        "    self.within_period_fc_layers = []\n",
        "    for channels in self.within_period_fc_channels:\n",
        "      self.within_period_fc_layers.append(layers.Dense(\n",
        "          channels, kernel_regularizer=regularizers.l2(self.l2_reg_weight),\n",
        "          activation=tf.nn.relu))\n",
        "    self.within_period_fc_layers.append(layers.Dense(\n",
        "        num_preds, kernel_regularizer=regularizers.l2(self.l2_reg_weight)))\n",
        "\n",
        "  def call(self, x):\n",
        "    # Ensures we are always using the right batch_size during train/eval.\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    # Conv Feature Extractor.\n",
        "    x = tf.reshape(x, [-1, self.image_size, self.image_size, 3])\n",
        "    #----------------Convolutional feature extractor Base ResNet50 Model----------------#\n",
        "    # 112x112x3 → 7x7x1024\n",
        "    x = self.base_model(x) #<base model이 ResNet-50\n",
        "    h = tf.shape(x)[1]\n",
        "    w = tf.shape(x)[2]\n",
        "    c = tf.shape(x)[3]\n",
        "    x = tf.reshape(x, [batch_size, -1, h, w, c])\n",
        "    #-------------------Temporal Context-------------------------#\n",
        "    # 3x3x3 512filters with RELU\n",
        "    # 3D Conv to give temporal context to per-frame embeddings. \n",
        "    for bn_layer, conv_layer in zip(self.temporal_bn_layers,\n",
        "                                    self.temporal_conv_layers):\n",
        "      x = conv_layer(x)\n",
        "      x = bn_layer(x)\n",
        "      x = tf.nn.relu(x)\n",
        "\n",
        "    #-----------------Dimensionality reduction--------------------#\n",
        "    x = tf.reduce_max(x, [2, 3])\n",
        "\n",
        "    # Reshape and prepare embs for output.\n",
        "    final_embs = x\n",
        "    #-----------------Get self-similarity matrix-----------------#\n",
        "    originTF.config.run_functions_eagerly(True)\n",
        "    x = get_sims(x, self.temperature)\n",
        "    #assert originTF.executing_eagerly(),'WHY.........'\n",
        "    x_np = np.reshape(x, (-1))\n",
        "    x_np = np.reshape(x_np,(self.num_frames,-1))\n",
        "    #sklearn.preprocessing 을 이용한 표준화\n",
        "    x_np = StandardScaler().fit_transform(x_np)\n",
        "    new_np=np.array(x_np)\n",
        "    plt.imshow(new_np, cmap='hot', interpolation='nearest')\n",
        "    plt.show()\n",
        "    print(new_np)\n",
        "\n",
        "    # #------count line 확인----#\n",
        "    # print('count line: ',new_np[138])\n",
        "    # line=new_np[138]\n",
        "    # for num in range(len(line)):\n",
        "    #   print(num,':',line[num])\n",
        "\n",
        "    #---------count 확인----------#\n",
        "    count_frames=[138, 150, 570, 1430, 2200, 2670, 3020,3370]\n",
        "    # for count_frame in count_frames:\n",
        "    #   x_np[count_frame][count_frame]=20\n",
        "    #   x_np[count_frame-1][count_frame]=20\n",
        "    #   x_np[count_frame][count_frame-1]=20\n",
        "    #   x_np[count_frame-1][count_frame-1]=20\n",
        "    # x_np=np.array(x_np)\n",
        "\n",
        "    \n",
        "    # plt.imshow(x_np, cmap='hot', interpolation='nearest')\n",
        "    # plt.show()\n",
        "\n",
        "\n",
        "    print(originTF.executing_eagerly())\n",
        "    #print(x.numpy())\n",
        "\n",
        "    # reduction = np.squeeze(x.numpy(), axis=1)\n",
        "    # print(reduction)\n",
        "    print('batch_size: ',batch_size)\n",
        "    print('num_frames: ',self.num_frames)\n",
        "    print('len(x_np)/batch_size:',len(x_np)/batch_size)\n",
        "##################################################################################\n",
        "\n",
        "\n",
        "    # # 3x3 conv layer on self-similarity matrix.\n",
        "    # x = self.conv_3x3_layer(x)\n",
        "    # x = tf.reshape(x, [batch_size, self.num_frames, -1])\n",
        "    # within_period_x = x\n",
        "\n",
        "    # # Period prediction.\n",
        "    # x = self.input_projection(x)\n",
        "    # x += self.pos_encoding\n",
        "    # for transformer_layer in self.transformer_layers:\n",
        "    #   x = transformer_layer(x)\n",
        "    # x = flatten_sequential_feats(x, batch_size, self.num_frames)\n",
        "    # for fc_layer in self.fc_layers:\n",
        "    #   x = self.dropout_layer(x)\n",
        "    #   x = fc_layer(x)\n",
        "\n",
        "    # # Within period prediction.\n",
        "    # within_period_x = self.input_projection2(within_period_x)\n",
        "    # within_period_x += self.pos_encoding2\n",
        "    # for transformer_layer in self.transformer_layers2:\n",
        "    #   within_period_x = transformer_layer(within_period_x)\n",
        "    # within_period_x = flatten_sequential_feats(within_period_x,\n",
        "    #                                            batch_size,\n",
        "    #                                            self.num_frames)\n",
        "    # for fc_layer in self.within_period_fc_layers:\n",
        "    #   within_period_x = self.dropout_layer(within_period_x)\n",
        "    #   within_period_x = fc_layer(within_period_x)\n",
        "\n",
        "    x,within_period_x,final_emb=1,2,3\n",
        "    return x, within_period_x, final_embs\n",
        "    \n",
        "\n",
        "#  @tf.function\n",
        "  def preprocess(self, imgs):\n",
        "    imgs = tf.cast(imgs, tf.float32)\n",
        "    imgs -= 127.5\n",
        "    imgs /= 127.5\n",
        "    imgs = tf.image.resize(imgs, (self.image_size, self.image_size))\n",
        "    return imgs\n",
        "\n",
        "\n",
        "def get_sims(embs, temperature):\n",
        "  \"\"\"Calculates self-similarity between batch of sequence of embeddings.\"\"\"\n",
        "  batch_size = tf.shape(embs)[0]\n",
        "  seq_len = tf.shape(embs)[1]\n",
        "  embs = tf.reshape(embs, [batch_size, seq_len, -1])\n",
        "\n",
        "  def _get_sims(embs):\n",
        "    \"\"\"Calculates self-similarity between sequence of embeddings.\"\"\"\n",
        "    dist = pairwise_l2_distance(embs, embs)\n",
        "    sims = -1.0 * dist\n",
        "    return sims\n",
        "\n",
        "  sims = tf.map_fn(_get_sims, embs)\n",
        "  sims /= temperature\n",
        "  sims = tf.nn.softmax(sims, axis=-1)\n",
        "  sims = tf.expand_dims(sims, -1)\n",
        "  return sims\n",
        "\n",
        "\n",
        "def flatten_sequential_feats(x, batch_size, seq_len):\n",
        "  \"\"\"Flattens sequential features with known batch size and seq_len.\"\"\"\n",
        "  x = tf.reshape(x, [batch_size, seq_len, -1])\n",
        "  return x\n",
        "\n",
        "\n",
        "# Transformer from https://www.tensorflow.org/tutorials/text/transformer .\n",
        "def scaled_dot_product_attention(q, k, v, mask):\n",
        "  \"\"\"Calculate the attention weights.\n",
        "\n",
        "  q, k, v must have matching leading dimensions.\n",
        "  k, v must have matching penultimate dimension, i.e.: seq_len_k = seq_len_v.\n",
        "  The mask has different shapes depending on its type(padding or look ahead)\n",
        "  but it must be broadcastable for addition.\n",
        "\n",
        "  Args:\n",
        "    q: query shape == (..., seq_len_q, depth)\n",
        "    k: key shape == (..., seq_len_k, depth)\n",
        "    v: value shape == (..., seq_len_v, depth_v)\n",
        "    mask: Float tensor with shape broadcastable\n",
        "          to (..., seq_len_q, seq_len_k). Defaults to None.\n",
        "\n",
        "  Returns:\n",
        "    outputs: shape == (..., seq_len_q, depth_v)\n",
        "    attention_weights: shape == (..., seq_len_q, seq_len_k)\n",
        "  \"\"\"\n",
        "\n",
        "  matmul_qk = tf.matmul(q, k, transpose_b=True)  # (..., seq_len_q, seq_len_k)\n",
        "\n",
        "  # scale matmul_qk.\n",
        "  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "  scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "  # add the mask to the scaled tensor.\n",
        "  if mask is not None:\n",
        "    scaled_attention_logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k) so that the scores\n",
        "  # add up to 1.\n",
        "  # (..., seq_len_q, seq_len_k)\n",
        "  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "\n",
        "  outputs = tf.matmul(attention_weights, v)  # (..., seq_len_q, depth_v)\n",
        "\n",
        "  return outputs, attention_weights\n",
        "\n",
        "\n",
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),\n",
        "      tf.keras.layers.Dense(d_model)\n",
        "  ])\n",
        "\n",
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  \"\"\"Multi-headed attention layer.\"\"\"\n",
        "\n",
        "  def __init__(self, d_model, num_heads):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.wq = tf.keras.layers.Dense(d_model)\n",
        "    self.wk = tf.keras.layers.Dense(d_model)\n",
        "    self.wv = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "  def split_heads(self, x, batch_size):\n",
        "    \"\"\"Split the last dimension into (num_heads, depth).\"\"\"\n",
        "    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, v, k, q, mask):\n",
        "    batch_size = tf.shape(q)[0]\n",
        "\n",
        "    q = self.wq(q)  # (batch_size, seq_len, d_model)\n",
        "    k = self.wk(k)  # (batch_size, seq_len, d_model)\n",
        "    v = self.wv(v)  # (batch_size, seq_len, d_model)\n",
        "\n",
        "    q = self.split_heads(\n",
        "        q, batch_size)  # (batch_size, num_heads, seq_len_q, depth)\n",
        "    k = self.split_heads(\n",
        "        k, batch_size)  # (batch_size, num_heads, seq_len_k, depth)\n",
        "    v = self.split_heads(\n",
        "        v, batch_size)  # (batch_size, num_heads, seq_len_v, depth)\n",
        "\n",
        "    # scaled_attention.shape == (batch_size, num_heads, seq_len_q, depth)\n",
        "    # attention_weights.shape == (batch_size, num_heads, seq_len_q, seq_len_k)\n",
        "    scaled_attention, attention_weights = scaled_dot_product_attention(\n",
        "        q, k, v, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(\n",
        "        scaled_attention,\n",
        "        perm=[0, 2, 1, 3])  # (batch_size, seq_len_q, num_heads, depth)\n",
        "\n",
        "    concat_attention = tf.reshape(\n",
        "        scaled_attention,\n",
        "        (batch_size, -1, self.d_model))  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    outputs = self.dense(concat_attention)  # (batch_size, seq_len_q, d_model)\n",
        "\n",
        "    return outputs, attention_weights\n",
        "\n",
        "\n",
        "class TransformerLayer(tf.keras.layers.Layer):\n",
        "  \"\"\"Implements a single transformer layer (https://arxiv.org/abs/1706.03762).\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, d_model, num_heads, dff,\n",
        "               dropout_rate=0.1,\n",
        "               reorder_ln=False):\n",
        "    super(TransformerLayer, self).__init__()\n",
        "\n",
        "    self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "    self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    self.reorder_ln = reorder_ln\n",
        "\n",
        "  def call(self, x):\n",
        "    inp_x = x\n",
        "\n",
        "    if self.reorder_ln:\n",
        "      x = self.layernorm1(x)\n",
        "\n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    attn_output, _ = self.mha(x, x, x, mask=None)\n",
        "    attn_output = self.dropout1(attn_output)\n",
        "\n",
        "    if self.reorder_ln:\n",
        "      out1 = inp_x + attn_output\n",
        "      x = out1\n",
        "    else:\n",
        "      # (batch_size, input_seq_len, d_model)\n",
        "      out1 = self.layernorm1(x + attn_output)\n",
        "      x = out1\n",
        "\n",
        "    if self.reorder_ln:\n",
        "      x = self.layernorm2(x)\n",
        "\n",
        "    # (batch_size, input_seq_len, d_model)\n",
        "    ffn_output = self.ffn(x)\n",
        "    ffn_output = self.dropout2(ffn_output)\n",
        "\n",
        "    if self.reorder_ln:\n",
        "      out2 = out1 + ffn_output\n",
        "    else:\n",
        "      # (batch_size, input_seq_len, d_model)\n",
        "      out2 = self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    return out2\n",
        "\n",
        "\n",
        "def pairwise_l2_distance(a, b):\n",
        "  \"\"\"Computes pairwise distances between all rows of a and all rows of b.\"\"\"\n",
        "  norm_a = tf.reduce_sum(tf.square(a), 1)\n",
        "  norm_a = tf.reshape(norm_a, [-1, 1])\n",
        "  norm_b = tf.reduce_sum(tf.square(b), 1)\n",
        "  norm_b = tf.reshape(norm_b, [1, -1])\n",
        "  dist = tf.maximum(norm_a - 2.0 * tf.matmul(a, b, False, True) + norm_b, 0.0)\n",
        "  return dist\n",
        "\n",
        "\n",
        "def get_repnet_model(logdir):\n",
        "  \"\"\"Returns a trained RepNet model.\n",
        "\n",
        "  Args:\n",
        "    logdir (string): Path to directory where checkpoint will be downloaded.\n",
        "\n",
        "  Returns:\n",
        "    model (Keras model): Trained RepNet model.\n",
        "  \"\"\"\n",
        "  # Check if we are in eager mode.\n",
        "  assert tf.executing_eagerly()\n",
        "\n",
        "  # Models will be called in eval mode.\n",
        "  tf.keras.backend.set_learning_phase(0)\n",
        "\n",
        "  # Define RepNet model.\n",
        "  model = ResnetPeriodEstimator()\n",
        "  # tf.function for speed.\n",
        "  #model.call = tf.function(model.call)\n",
        "\n",
        "  # # Define checkpoint and checkpoint manager.\n",
        "  # ckpt = tf.train.Checkpoint(model=model)\n",
        "  # ckpt_manager = tf.train.CheckpointManager(\n",
        "  #     ckpt, directory=logdir, max_to_keep=10)\n",
        "  # latest_ckpt = ckpt_manager.latest_checkpoint\n",
        "  # print('Loading from: ', latest_ckpt)\n",
        "  # if not latest_ckpt:\n",
        "  #   raise ValueError('Path does not have a checkpoint to load.')\n",
        "  # # Restore weights.\n",
        "  # print(\"Restore weights\")\n",
        "  # ckpt.restore(latest_ckpt).expect_partial()\n",
        "  # print('nice restored!')\n",
        "  # # Pass dummy frames to build graph.\n",
        "  model(tf.random.uniform((1, FRAME, 112, 112, 3)))\n",
        "  return model\n",
        "\n",
        "\n",
        "def unnorm(query_frame):\n",
        "  min_v = query_frame.min()\n",
        "  max_v = query_frame.max()\n",
        "  query_frame = (query_frame - min_v) / max(1e-7, (max_v - min_v))\n",
        "  return query_frame\n",
        "\n",
        "\n",
        "def create_count_video(frames,\n",
        "                       per_frame_counts,\n",
        "                       within_period,\n",
        "                       score,\n",
        "                       fps,\n",
        "                       output_file,\n",
        "                       delay,\n",
        "                       plot_count=True,\n",
        "                       plot_within_period=False,\n",
        "                       plot_score=False):\n",
        "  \"\"\"Creates video with running count and within period predictions.\n",
        "\n",
        "  Args:\n",
        "    frames (List): List of images in form of NumPy arrays.\n",
        "    per_frame_counts (List): List of floats indicating repetition count for\n",
        "      each frame. This is the rate of repetition for that particular frame.\n",
        "      Summing this list up gives count over entire video.\n",
        "    within_period (List): List of floats indicating score between 0 and 1 if the\n",
        "      frame is inside the periodic/repeating portion of a video or not.\n",
        "    score (float): Score between 0 and 1 indicating the confidence of the\n",
        "      RepNet model's count predictions.\n",
        "    fps (int): Frames per second of the input video. Used to scale the\n",
        "      repetition rate predictions to Hz.\n",
        "    output_file (string): Path of the output video.\n",
        "    delay (integer): Delay between each frame in the output video.\n",
        "    plot_count (boolean): if True plots the count in the output video.\n",
        "    plot_within_period (boolean): if True plots the per-frame within period\n",
        "      scores.\n",
        "    plot_score (boolean): if True plots the confidence of the model along with\n",
        "      count ot within_period scores.\n",
        "  \"\"\"\n",
        "  if output_file[-4:] not in ['.mp4', '.gif']:\n",
        "    raise ValueError('Output format can only be mp4 or gif')\n",
        "  num_frames = len(frames)\n",
        "\n",
        "  running_counts = np.cumsum(per_frame_counts)\n",
        "  final_count = running_counts[-1]\n",
        "\n",
        "  def count(idx):\n",
        "    return int(np.round(running_counts[idx]))\n",
        "\n",
        "  def rate(idx):\n",
        "    return per_frame_counts[idx] * fps\n",
        "\n",
        "  if plot_count and not plot_within_period:\n",
        "    fig = plt.figure(figsize=(10, 12), tight_layout=True)\n",
        "    im = plt.imshow(unnorm(frames[0]))\n",
        "    if plot_score:\n",
        "      plt.suptitle('Pred Count: %d, '\n",
        "                   'Prob: %0.1f' % (int(np.around(final_count)), score),\n",
        "                   fontsize=24)\n",
        "\n",
        "    plt.title('Count 0, Rate: 0', fontsize=24)\n",
        "    plt.axis('off')\n",
        "    plt.grid(b=None)\n",
        "    def update_count_plot(i):\n",
        "      \"\"\"Updates the count plot.\"\"\"\n",
        "      im.set_data(unnorm(frames[i]))\n",
        "      plt.title('Count %d, Rate: %0.4f Hz' % (count(i), rate(i)), fontsize=24)\n",
        "\n",
        "    anim = FuncAnimation(\n",
        "        fig,\n",
        "        update_count_plot,\n",
        "        frames=np.arange(1, num_frames),\n",
        "        interval=delay,\n",
        "        blit=False)\n",
        "    if output_file[-3:] == 'mp4':\n",
        "      anim.save(output_file, dpi=100, fps=24)\n",
        "    elif output_file[-3:] == 'gif':\n",
        "      anim.save(output_file, writer='imagemagick', fps=24, dpi=100)\n",
        "\n",
        "  elif plot_within_period:\n",
        "    fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
        "    im = axs[0].imshow(unnorm(frames[0]))\n",
        "    axs[1].plot(0, within_period[0])\n",
        "    axs[1].set_xlim((0, len(frames)))\n",
        "    axs[1].set_ylim((0, 1))\n",
        "\n",
        "    if plot_score:\n",
        "      plt.suptitle('Pred Count: %d, '\n",
        "                   'Prob: %0.1f' % (int(np.around(final_count)), score),\n",
        "                   fontsize=24)\n",
        "\n",
        "    if plot_count:\n",
        "      axs[0].set_title('Count 0, Rate: 0', fontsize=20)\n",
        "\n",
        "    plt.axis('off')\n",
        "    plt.grid(b=None)\n",
        "\n",
        "    def update_within_period_plot(i):\n",
        "      \"\"\"Updates the within period plot along with count.\"\"\"\n",
        "      im.set_data(unnorm(frames[i]))\n",
        "      axs[0].set_xticks([])\n",
        "      axs[0].set_yticks([])\n",
        "      xs = []\n",
        "      ys = []\n",
        "      if plot_count:\n",
        "        axs[0].set_title('Count %d, Rate: %0.4f Hz' % (count(i), rate(i)),\n",
        "                         fontsize=20)\n",
        "      for idx in range(i):\n",
        "        xs.append(idx)\n",
        "        ys.append(within_period[int(idx * len(within_period) / num_frames)])\n",
        "      axs[1].clear()\n",
        "      axs[1].set_title('Within Period or Not', fontsize=20)\n",
        "      axs[1].set_xlim((0, num_frames))\n",
        "      axs[1].set_ylim((-0.05, 1.05))\n",
        "      axs[1].plot(xs, ys)\n",
        "\n",
        "    anim = FuncAnimation(\n",
        "        fig,\n",
        "        update_within_period_plot,\n",
        "        frames=np.arange(1, num_frames),\n",
        "        interval=delay,\n",
        "        blit=False,\n",
        "    )\n",
        "    if output_file[-3:] == 'mp4':\n",
        "      anim.save(output_file, dpi=100, fps=24)\n",
        "    elif output_file[-3:] == 'gif':\n",
        "      anim.save(output_file, writer='imagemagick', fps=24, dpi=100)\n",
        "\n",
        "  plt.close()\n",
        "\n",
        "\n",
        "def show_video(video_path):\n",
        "  mp4 = open(video_path, 'rb').read()\n",
        "  data_url = 'data:video/mp4;base64,' + base64.b64encode(mp4).decode()\n",
        "  return HTML(\"\"\"<video width=600 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\"></video>\n",
        "  \"\"\" % data_url)\n",
        "\n",
        "\n",
        "def viz_reps(frames,\n",
        "             count,\n",
        "             score,\n",
        "             alpha=1.0,\n",
        "             pichart=True,\n",
        "             colormap=plt.cm.PuBu,\n",
        "             num_frames=None,\n",
        "             interval=30,\n",
        "             plot_score=True):\n",
        "  \"\"\"Visualize repetitions.\"\"\"\n",
        "  if isinstance(count, list):\n",
        "    counts = len(frames) * [count/len(frames)]\n",
        "  else:\n",
        "    counts = count\n",
        "  sum_counts = np.cumsum(counts)\n",
        "  tmp_path = '/tmp/output.mp4'\n",
        "  fig, ax = plt.subplots(ncols=1, nrows=1, figsize=(5, 5),\n",
        "                         tight_layout=True,)\n",
        "\n",
        "  h, w, _ = np.shape(frames[0])\n",
        "  wedge_x = 95 / 112 * w\n",
        "  wedge_y = 17 / 112 * h\n",
        "  wedge_r = 15 / 112 * h\n",
        "  txt_x = 95 / 112 * w\n",
        "  txt_y = 19 / 112 * h\n",
        "  otxt_size = 62 / 112 * h\n",
        "\n",
        "  if plot_score:\n",
        "    plt.title('Score:%.2f' % score, fontsize=20)\n",
        "  im0 = ax.imshow(unnorm(frames[0]))\n",
        "\n",
        "  if not num_frames:\n",
        "    num_frames = len(frames)\n",
        "\n",
        "  if pichart:\n",
        "    wedge1 = matplotlib.patches.Wedge(\n",
        "        center=(wedge_x, wedge_y),\n",
        "        r=wedge_r,\n",
        "        theta1=0,\n",
        "        theta2=0,\n",
        "        color=colormap(1.),\n",
        "        alpha=alpha)\n",
        "    wedge2 = matplotlib.patches.Wedge(\n",
        "        center=(wedge_x, wedge_y),\n",
        "        r=wedge_r,\n",
        "        theta1=0,\n",
        "        theta2=0,\n",
        "        color=colormap(0.5),\n",
        "        alpha=alpha)\n",
        "\n",
        "    ax.add_patch(wedge1)\n",
        "    ax.add_patch(wedge2)\n",
        "    txt = ax.text(\n",
        "        txt_x,\n",
        "        txt_y,\n",
        "        '0',\n",
        "        size=35,\n",
        "        ha='center',\n",
        "        va='center',\n",
        "        alpha=0.9,\n",
        "        color='white',\n",
        "    )\n",
        "\n",
        "  else:\n",
        "    txt = ax.text(\n",
        "        txt_x,\n",
        "        txt_y,\n",
        "        '0',\n",
        "        size=otxt_size,\n",
        "        ha='center',\n",
        "        va='center',\n",
        "        alpha=0.8,\n",
        "        color=colormap(0.4),\n",
        "    )\n",
        "\n",
        "  def update(i):\n",
        "    \"\"\"Update plot with next frame.\"\"\"\n",
        "    im0.set_data(unnorm(frames[i]))\n",
        "    ctr = int(sum_counts[i])\n",
        "    if pichart:\n",
        "      if ctr%2 == 0:\n",
        "        wedge1.set_color(colormap(1.0))\n",
        "        wedge2.set_color(colormap(0.5))\n",
        "      else:\n",
        "        wedge1.set_color(colormap(0.5))\n",
        "        wedge2.set_color(colormap(1.0))\n",
        "\n",
        "      wedge1.set_theta1(-90)\n",
        "      wedge1.set_theta2(-90 - 360 * (1 - sum_counts[i] % 1.0))\n",
        "      wedge2.set_theta1(-90 - 360 * (1 - sum_counts[i] % 1.0))\n",
        "      wedge2.set_theta2(-90)\n",
        "\n",
        "    txt.set_text(int(sum_counts[i]))\n",
        "    ax.grid(False)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    plt.tight_layout()\n",
        "\n",
        "  anim = FuncAnimation(\n",
        "      fig,\n",
        "      update,\n",
        "      frames=num_frames,\n",
        "      interval=interval,\n",
        "      blit=False)\n",
        "  anim.save(tmp_path, dpi=80)\n",
        "  plt.close()\n",
        "  return show_video(tmp_path)\n",
        "\n",
        "\n",
        "def record_video(interval_in_ms, num_frames, quality=0.8):\n",
        "  \"\"\"Capture video from webcam.\"\"\"\n",
        "  # https://colab.research.google.com/notebooks/snippets/advanced_outputs.ipynb.\n",
        "\n",
        "  # Give warning before recording.\n",
        "  for i in range(0, 3):\n",
        "    print('Opening webcam in %d seconds'%(3-i))\n",
        "    time.sleep(1)\n",
        "    output.clear('status_text')\n",
        "\n",
        "  js = Javascript('''\n",
        "    async function recordVideo(interval_in_ms, num_frames, quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      // show the video in the HTML element\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight,\n",
        "        true);\n",
        "\n",
        "      for (let i = 0; i < num_frames; i++) {\n",
        "        const canvas = document.createElement('canvas');\n",
        "        canvas.width = video.videoWidth;\n",
        "        canvas.height = video.videoHeight;\n",
        "        canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "        img = canvas.toDataURL('image/jpeg', quality);\n",
        "        google.colab.kernel.invokeFunction(\n",
        "        'notebook.get_webcam_video', [img], {});\n",
        "        await new Promise(resolve => setTimeout(resolve, interval_in_ms));\n",
        "      }\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  eval_js('recordVideo({},{},{})'.format(interval_in_ms, num_frames, quality))\n",
        "\n",
        "\n",
        "def data_uri_to_img(uri):\n",
        "  \"\"\"Convert base64image to Numpy array.\"\"\"\n",
        "  image = base64.b64decode(uri.split(',')[1], validate=True)\n",
        "  # Binary string to PIL image.\n",
        "  image = Image.open(io.BytesIO(image))\n",
        "  image = image.resize((224, 224))\n",
        "  # PIL to Numpy array.\n",
        "  image = np.array(np.array(image, dtype=np.uint8), np.float32)\n",
        "  return image\n",
        "\n",
        "\n",
        "def read_video(video_filename, width=224, height=224):\n",
        "  \"\"\"Read video from file.\"\"\"\n",
        "  cap = cv2.VideoCapture(video_filename)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  frames = []\n",
        "  if cap.isOpened():\n",
        "    while True:\n",
        "      success, frame_bgr = cap.read()\n",
        "      if not success:\n",
        "        break\n",
        "      frame_rgb = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
        "      frame_rgb = cv2.resize(frame_rgb, (width, height))\n",
        "      frames.append(frame_rgb)\n",
        "  frames = np.asarray(frames)\n",
        "  return frames, fps\n",
        "\n",
        "\n",
        "def get_webcam_video(img_b64):\n",
        "  \"\"\"Populates global variable imgs by converting image URI to Numpy array.\"\"\"\n",
        "  image = data_uri_to_img(img_b64)\n",
        "  imgs.append(image)\n",
        "\n",
        "\n",
        "def download_video_from_url(url_to_video,\n",
        "                            path_to_video='/tmp/video.mp4'):\n",
        "  if os.path.exists(path_to_video):\n",
        "    os.remove(path_to_video)\n",
        "  ydl_opts = {\n",
        "      'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/mp4',\n",
        "      'outtmpl': str(path_to_video),\n",
        "  }\n",
        "  with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n",
        "    ydl.download([url_to_video])\n",
        "\n",
        "\n",
        "def get_score(period_score, within_period_score):\n",
        "  \"\"\"Combine the period and periodicity scores.\"\"\"\n",
        "  within_period_score = tf.nn.sigmoid(within_period_score)[:, 0]\n",
        "  per_frame_periods = tf.argmax(period_score, axis=-1) + 1\n",
        "  pred_period_conf = tf.reduce_max(\n",
        "      tf.nn.softmax(period_score, axis=-1), axis=-1)\n",
        "  pred_period_conf = tf.where(\n",
        "      tf.math.less(per_frame_periods, 3), 0.0, pred_period_conf)\n",
        "  within_period_score *= pred_period_conf\n",
        "  within_period_score = np.sqrt(within_period_score)\n",
        "  pred_score = tf.reduce_mean(within_period_score)\n",
        "  return pred_score, within_period_score\n",
        "\n",
        "\n",
        "def get_counts(model, frames, strides, batch_size,\n",
        "               threshold,\n",
        "               within_period_threshold,\n",
        "               constant_speed=False,\n",
        "               median_filter=False,\n",
        "               fully_periodic=False):\n",
        "  \"\"\"Pass frames through model and conver period predictions to count.\"\"\"\n",
        "  seq_len = len(frames)\n",
        "  print('frame number: ',seq_len)\n",
        "  raw_scores_list = []\n",
        "  scores = []\n",
        "  within_period_scores_list = []\n",
        "\n",
        "  if fully_periodic:\n",
        "    within_period_threshold = 0.0\n",
        "  \n",
        "  #resize processing\n",
        "  frames = model.preprocess(frames) \n",
        "\n",
        "  #strides=[1,2,3,4]\n",
        "  for stride in strides:\n",
        "    print('################')\n",
        "    print('stride:',stride)\n",
        "    num_batches = int(np.ceil(seq_len/model.num_frames/stride/batch_size))\n",
        "    print(num_batches)\n",
        "    raw_scores_per_stride = []\n",
        "    within_period_score_stride = []\n",
        "    for batch_idx in range(num_batches):\n",
        "      print(\"batch index: \",batch_idx)\n",
        "      idxes = tf.range(batch_idx*batch_size*model.num_frames*stride,\n",
        "                       (batch_idx+1)*batch_size*model.num_frames*stride,\n",
        "                       stride)\n",
        "      idxes = tf.clip_by_value(idxes, 0, seq_len-1)\n",
        "      curr_frames = tf.gather(frames, idxes)\n",
        "      curr_frames = tf.reshape(\n",
        "          curr_frames,\n",
        "          [batch_size, model.num_frames, model.image_size, model.image_size, 3])\n",
        "      raw_scores, within_period_scores, _ = model(curr_frames)\n",
        "      \n",
        "  return 1,1,1,1,1\n",
        "\n",
        "\n",
        "  #     raw_scores_per_stride.append(np.reshape(raw_scores.numpy(),\n",
        "  #                                             [-1, model.num_frames//2]))\n",
        "  #     within_period_score_stride.append(np.reshape(within_period_scores.numpy(),\n",
        "  #                                                  [-1, 1]))\n",
        "  #   raw_scores_per_stride = np.concatenate(raw_scores_per_stride, axis=0)\n",
        "  #   raw_scores_list.append(raw_scores_per_stride)\n",
        "  #   within_period_score_stride = np.concatenate(\n",
        "  #       within_period_score_stride, axis=0)\n",
        "  #   pred_score, within_period_score_stride = get_score(\n",
        "  #       raw_scores_per_stride, within_period_score_stride)\n",
        "  #   scores.append(pred_score)\n",
        "  #   within_period_scores_list.append(within_period_score_stride)\n",
        "\n",
        "  # # Stride chooser\n",
        "  # argmax_strides = np.argmax(scores)\n",
        "  # chosen_stride = strides[argmax_strides]\n",
        "  # raw_scores = np.repeat(\n",
        "  #     raw_scores_list[argmax_strides], chosen_stride, axis=0)[:seq_len]\n",
        "  # within_period = np.repeat(\n",
        "  #     within_period_scores_list[argmax_strides], chosen_stride,\n",
        "  #     axis=0)[:seq_len]\n",
        "  # within_period_binary = np.asarray(within_period > within_period_threshold)\n",
        "  # if median_filter:\n",
        "  #   within_period_binary = medfilt(within_period_binary, 5)\n",
        "\n",
        "  # # Select Periodic frames\n",
        "  # periodic_idxes = np.where(within_period_binary)[0]\n",
        "\n",
        "  # if constant_speed:\n",
        "  #   # Count by averaging predictions. Smoother but\n",
        "  #   # assumes constant speed.\n",
        "  #   scores = tf.reduce_mean(\n",
        "  #       tf.nn.softmax(raw_scores[periodic_idxes], axis=-1), axis=0)\n",
        "  #   max_period = np.argmax(scores)\n",
        "  #   pred_score = scores[max_period]\n",
        "  #   pred_period = chosen_stride * (max_period + 1)\n",
        "  #   per_frame_counts = (\n",
        "  #       np.asarray(seq_len * [1. / pred_period]) *\n",
        "  #       np.asarray(within_period_binary))\n",
        "  # else:\n",
        "  #   # Count each frame. More noisy but adapts to changes in speed.\n",
        "  #   pred_score = tf.reduce_mean(within_period)\n",
        "  #   per_frame_periods = tf.argmax(raw_scores, axis=-1) + 1\n",
        "  #   per_frame_counts = tf.where(\n",
        "  #       tf.math.less(per_frame_periods, 3),\n",
        "  #       0.0,\n",
        "  #       tf.math.divide(1.0,\n",
        "  #                      tf.cast(chosen_stride * per_frame_periods, tf.float32)),\n",
        "  #   )\n",
        "  #   if median_filter:\n",
        "  #     per_frame_counts = medfilt(per_frame_counts, 5)\n",
        "\n",
        "  #   per_frame_counts *= np.asarray(within_period_binary)\n",
        "\n",
        "  #   pred_period = seq_len/np.sum(per_frame_counts)\n",
        "\n",
        "  # if pred_score < threshold:\n",
        "  #   print('No repetitions detected in video as score '\n",
        "  #         '%0.2f is less than threshold %0.2f.'%(pred_score, threshold))\n",
        "  #   per_frame_counts = np.asarray(len(per_frame_counts) * [0.])\n",
        "\n",
        "  # return (pred_period, pred_score, within_period,\n",
        "  #         per_frame_counts, chosen_stride)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPpgGXG_aalo"
      },
      "source": [
        "## Load trained RepNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "c3-XygLIxwBK",
        "outputId": "4abd28ce-2153-4a27-d490-025e80c755db"
      },
      "source": [
        "PATH_TO_CKPT = '/tmp/repnet_ckpt/'\n",
        "!mkdir $PATH_TO_CKPT\n",
        "!wget -nc -P $PATH_TO_CKPT https://storage.googleapis.com/repnet_ckpt/checkpoint\n",
        "!wget -nc -P $PATH_TO_CKPT https://storage.googleapis.com/repnet_ckpt/ckpt-88.data-00000-of-00002\n",
        "!wget -nc -P $PATH_TO_CKPT https://storage.googleapis.com/repnet_ckpt/ckpt-88.data-00001-of-00002\n",
        "!wget -nc -P $PATH_TO_CKPT https://storage.googleapis.com/repnet_ckpt/ckpt-88.index\n",
        "\n",
        "model = get_repnet_model(PATH_TO_CKPT)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/tmp/repnet_ckpt/’: File exists\n",
            "File ‘/tmp/repnet_ckpt/checkpoint’ already there; not retrieving.\n",
            "\n",
            "File ‘/tmp/repnet_ckpt/ckpt-88.data-00000-of-00002’ already there; not retrieving.\n",
            "\n",
            "File ‘/tmp/repnet_ckpt/ckpt-88.data-00001-of-00002’ already there; not retrieving.\n",
            "\n",
            "File ‘/tmp/repnet_ckpt/ckpt-88.index’ already there; not retrieving.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/backend.py:401: UserWarning: `tf.keras.backend.set_learning_phase` is deprecated and will be removed after 2020-10-11. To update it, simply pass a True/False value to the `training` argument of the `__call__` method of your layer or model.\n",
            "  warnings.warn('`tf.keras.backend.set_learning_phase` is deprecated and '\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n",
            "True\n",
            "batch_size:  tf.Tensor(1, shape=(), dtype=int32)\n",
            "num_frames:  3950\n",
            "len(x_np)/batch_size: tf.Tensor(3950.0, shape=(), dtype=float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CNR6sVATy8yX"
      },
      "source": [
        "## Set Params\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PaCB432zUrZu"
      },
      "source": [
        "##@title \n",
        "\n",
        "# FPS while recording video from webcam.\n",
        "WEBCAM_FPS = 16#@param {type:\"integer\"}\n",
        "\n",
        "# Time in seconds to record video on webcam. \n",
        "RECORDING_TIME_IN_SECONDS = 8. #@param {type:\"number\"}\n",
        "\n",
        "# Threshold to consider periodicity in entire video.\n",
        "THRESHOLD = 0.5#@param {type:\"number\"}\n",
        "\n",
        "# Threshold to consider periodicity for individual frames in video.\n",
        "WITHIN_PERIOD_THRESHOLD = 0.3#@param {type:\"number\"}\n",
        "\n",
        "# Use this setting for better results when it is \n",
        "# known action is repeating at constant speed.\n",
        "CONSTANT_SPEED = True#@param {type:\"boolean\"}\n",
        "\n",
        "# Use median filtering in time to ignore noisy frames.\n",
        "MEDIAN_FILTER = True#@param {type:\"boolean\"}\n",
        "\n",
        "# Use this setting for better results when it is \n",
        "# known the entire video is periodic/reapeating and\n",
        "# has no aperiodic frames.\n",
        "FULLY_PERIODIC = False#@param {type:\"boolean\"}\n",
        "\n",
        "# Plot score in visualization video.\n",
        "PLOT_SCORE = False#@param {type:\"boolean\"}\n",
        "\n",
        "# Visualization video's FPS.\n",
        "VIZ_FPS = 30#@param {type:\"integer\"}"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clHlZdUuTzHF"
      },
      "source": [
        "# Get input video\n",
        "\n",
        "We provide 3 ways to get input video:\n",
        "1. upload video to your Google Drive.\n",
        "2. provide URL of a video.\n",
        "3. record video using webcam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFC7Y-egr-8E"
      },
      "source": [
        "## Get Video from URL\n",
        "\n",
        "Provide a link to mp4/gif hosted online."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IScNPW9C2Urp"
      },
      "source": [
        "imgs, vid_fps = read_video(\"drive/MyDrive/repnet/20201123_180317_counting_region_croped.mp4\")\n",
        "#show_video(\"drive/MyDrive/repnet/20201110_120543_croped_half_faster.mp4\")"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKWkGlEsa3Tg"
      },
      "source": [
        "# Run RepNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUg2vSYhmsT0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "a1cc6ead-d097-433a-ba34-882a4e8fcbf5"
      },
      "source": [
        "print('Running RepNet...') \n",
        "(pred_period, pred_score, within_period,\n",
        " per_frame_counts, chosen_stride) = get_counts(\n",
        "     model,\n",
        "     imgs,\n",
        "     strides=[1],\n",
        "     batch_size=1,\n",
        "     threshold=THRESHOLD,\n",
        "     within_period_threshold=WITHIN_PERIOD_THRESHOLD,\n",
        "     constant_speed=CONSTANT_SPEED,\n",
        "     median_filter=MEDIAN_FILTER,\n",
        "     fully_periodic=FULLY_PERIODIC)\n",
        "print('END')\n",
        "#print('Visualizing results...') \n",
        "# viz_reps(imgs, per_frame_counts, pred_score, interval=1000/VIZ_FPS,\n",
        "#          plot_score=PLOT_SCORE)\n"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running RepNet...\n",
            "frame number:  3950\n",
            "################\n",
            "stride: 1\n",
            "1\n",
            "batch index:  0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-32907a4e6da7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m      \u001b[0mconstant_speed\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONSTANT_SPEED\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m      \u001b[0mmedian_filter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMEDIAN_FILTER\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m      fully_periodic=FULLY_PERIODIC)\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'END'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m#print('Visualizing results...')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-b36de2dc9cff>\u001b[0m in \u001b[0;36mget_counts\u001b[0;34m(model, frames, strides, batch_size, threshold, within_period_threshold, constant_speed, median_filter, fully_periodic)\u001b[0m\n\u001b[1;32m    862\u001b[0m           \u001b[0mcurr_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m           [batch_size, model.num_frames, model.image_size, model.image_size, 3])\n\u001b[0;32m--> 864\u001b[0;31m       \u001b[0mraw_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwithin_period_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurr_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-47-b36de2dc9cff>\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;31m#----------------Convolutional feature extractor Base ResNet50 Model----------------#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;31m# 112x112x3 → 7x7x1024\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#<base model이 ResNet-50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    413\u001b[0m     \"\"\"\n\u001b[1;32m    414\u001b[0m     return self._run_internal_graph(\n\u001b[0;32m--> 415\u001b[0;31m         inputs, training=training, mask=mask)\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/functional.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_arguments\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1035\u001b[0m         with autocast_variable.enable_auto_cast_variables(\n\u001b[1;32m   1036\u001b[0m             self._compute_dtype_object):\n\u001b[0;32m-> 1037\u001b[0;31m           \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activity_regularizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/layers/core.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/activations.py\u001b[0m in \u001b[0;36mswish\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[0;34m-\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mRamachandran\u001b[0m \u001b[0met\u001b[0m \u001b[0mal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2017\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0marxiv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morg\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m1710.05941\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m   \"\"\"\n\u001b[0;32m--> 270\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *a, **k)\u001b[0m\n\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36mdecorated\u001b[0;34m(wrapped, args, kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_eager_mode_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_graph_mode_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/custom_gradient.py\u001b[0m in \u001b[0;36m_eager_mode_decorator\u001b[0;34m(f, args, kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m   \u001b[0;34m\"\"\"Implement custom gradient decorator for eager mode.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mtape_lib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariableWatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mvariable_watcher\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m   \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m   \u001b[0mall_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_impl.py\u001b[0m in \u001b[0;36mswish\u001b[0;34m(features)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdy\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mactivation_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mbinary_op_wrapper\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;31m#   r_binary_op_wrapper use different force_same_dtype values.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_promote_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_same_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1368\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m         \u001b[0;31m# Even if dispatching the op failed, the RHS may be a tensor aware\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36m_mul_dispatch\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   1708\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msparse_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparseTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmultiply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py\u001b[0m in \u001b[0;36mmultiply\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m    528\u001b[0m   \"\"\"\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgen_math_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_math_ops.py\u001b[0m in \u001b[0;36mmul\u001b[0;34m(x, y, name)\u001b[0m\n\u001b[1;32m   6234\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6235\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6236\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6237\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6238\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: failed to allocate memory [Op:Mul]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYcthjnIJC3P"
      },
      "source": [
        "# Debugging video showing scores, per-frame frequency prediction and \n",
        "# within_period scores.\n",
        "\n",
        "# create_count_video(imgs,\n",
        "#                    per_frame_counts,\n",
        "#                    within_period,\n",
        "#                    score=pred_score,\n",
        "#                    fps=vid_fps,\n",
        "#                    output_file='/tmp/debug_video.mp4',\n",
        "#                    delay=1000/VIZ_FPS,\n",
        "#                    plot_count=True,\n",
        "#                    plot_within_period=True,\n",
        "#                    plot_score=True)\n",
        "# show_video('/tmp/debug_video.mp4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGFXFGjzzt5U"
      },
      "source": [
        "# Citation\n",
        "\n",
        "If you found our paper/code useful in your research, consider citing our paper:\n",
        "\n",
        "\n",
        "```\n",
        "@InProceedings{Dwibedi_2020_CVPR,\n",
        "author = {Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},\n",
        "title = {Counting Out Time: Class Agnostic Video Repetition Counting in the Wild},\n",
        "booktitle = {IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},\n",
        "month = {June},\n",
        "year = {2020}\n",
        "}\n",
        "```\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}